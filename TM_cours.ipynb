{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQXEjOLMu8uKSGYxcAbDhp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/balkisoues/text_mining/blob/main/TM_cours.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Tokenization**\n",
        "\n"
      ],
      "metadata": {
        "id": "rNvm36mDh0gn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-D03aJAbxIC",
        "outputId": "5ff7c69e-5409-469a-d48f-3c39c3e7c37b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "text = 'This command lists all files that Git is currently tracking in your repository, including those in subdirectories. To check what files are in a Git repository (including those on GitHub, once cloned locally), you can use the following commands in your terminal or command prompt. Note: To use these commands, you need to have Git installed and be within the local clone of your GitHub repository in your terminal.'"
      ],
      "metadata": {
        "id": "XHrHbShNhD4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsgTmw85hXZW",
        "outputId": "f1aab8b0-255c-496e-9586-398630478196"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This command lists all files that Git is currently tracking in your repository, including those in subdirectories.',\n",
              " 'To check what files are in a Git repository (including those on GitHub, once cloned locally), you can use the following commands in your terminal or command prompt.',\n",
              " 'Note: To use these commands, you need to have Git installed and be within the local clone of your GitHub repository in your terminal.']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "text = \"Let us understand the difference between sentence & word tokenizer. It is going to be a simple example.\"\n",
        "\n",
        "sent_tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TY3IgI3jhghX",
        "outputId": "f65c0d89-e833-4679-efbd-30d4c0a279ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Let us understand the difference between sentence & word tokenizer.',\n",
              " 'It is going to be a simple example.']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "word_tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aju-ysuUh-OX",
        "outputId": "3c427d3f-c517-4cc2-d7cb-9810dab42bd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This',\n",
              " 'command',\n",
              " 'lists',\n",
              " 'all',\n",
              " 'files',\n",
              " 'that',\n",
              " 'Git',\n",
              " 'is',\n",
              " 'currently',\n",
              " 'tracking',\n",
              " 'in',\n",
              " 'your',\n",
              " 'repository',\n",
              " ',',\n",
              " 'including',\n",
              " 'those',\n",
              " 'in',\n",
              " 'subdirectories',\n",
              " '.',\n",
              " 'To',\n",
              " 'check',\n",
              " 'what',\n",
              " 'files',\n",
              " 'are',\n",
              " 'in',\n",
              " 'a',\n",
              " 'Git',\n",
              " 'repository',\n",
              " '(',\n",
              " 'including',\n",
              " 'those',\n",
              " 'on',\n",
              " 'GitHub',\n",
              " ',',\n",
              " 'once',\n",
              " 'cloned',\n",
              " 'locally',\n",
              " ')',\n",
              " ',',\n",
              " 'you',\n",
              " 'can',\n",
              " 'use',\n",
              " 'the',\n",
              " 'following',\n",
              " 'commands',\n",
              " 'in',\n",
              " 'your',\n",
              " 'terminal',\n",
              " 'or',\n",
              " 'command',\n",
              " 'prompt',\n",
              " '.',\n",
              " 'Note',\n",
              " ':',\n",
              " 'To',\n",
              " 'use',\n",
              " 'these',\n",
              " 'commands',\n",
              " ',',\n",
              " 'you',\n",
              " 'need',\n",
              " 'to',\n",
              " 'have',\n",
              " 'Git',\n",
              " 'installed',\n",
              " 'and',\n",
              " 'be',\n",
              " 'within',\n",
              " 'the',\n",
              " 'local',\n",
              " 'clone',\n",
              " 'of',\n",
              " 'your',\n",
              " 'GitHub',\n",
              " 'repository',\n",
              " 'in',\n",
              " 'your',\n",
              " 'terminal',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Filter stop words**"
      ],
      "metadata": {
        "id": "VpnRlXe6iTP5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "print (\" english stop words \" , stopwords.words('english') )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2Kf4uXjiCA8",
        "outputId": "91cc51b3-4067-4e5a-9df8-c13762ec4a98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " english stop words  ['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Applying stop words removal to text**"
      ],
      "metadata": {
        "id": "bc6igZOqi_Lk"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbb18de7",
        "outputId": "dcfd1dd8-d0b8-48aa-d080-df42189ed9ce"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Tokenize the text into words\n",
        "words = word_tokenize(text)\n",
        "\n",
        "# Get English stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Filter out stop words\n",
        "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "print(\"Original words:\", words)\n",
        "print(\"Filtered words (after stop word removal):\")\n",
        "print(filtered_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original words: ['this', 'command', 'lists', 'all', 'files', 'that', 'git', 'is', 'currently', 'tracking', 'in', 'your', 'repository', ',', 'including', 'those', 'in', 'subdirectories', '.', 'to', 'check', 'what', 'files', 'are', 'in', 'a', 'git', 'repository', '(', 'including', 'those', 'on', 'github', ',', 'once', 'cloned', 'locally', ')', ',', 'you', 'can', 'use', 'the', 'following', 'commands', 'in', 'your', 'terminal', 'or', 'command', 'prompt', '.', 'note', ':', 'to', 'use', 'these', 'commands', ',', 'you', 'need', 'to', 'have', 'git', 'installed', 'and', 'be', 'within', 'the', 'local', 'clone', 'of', 'your', 'github', 'repository', 'in', 'your', 'terminal', '.']\n",
            "Filtered words (after stop word removal):\n",
            "['command', 'lists', 'files', 'git', 'currently', 'tracking', 'repository', ',', 'including', 'subdirectories', '.', 'check', 'files', 'git', 'repository', '(', 'including', 'github', ',', 'cloned', 'locally', ')', ',', 'use', 'following', 'commands', 'terminal', 'command', 'prompt', '.', 'note', ':', 'use', 'commands', ',', 'need', 'git', 'installed', 'within', 'local', 'clone', 'github', 'repository', 'terminal', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Capitalization**"
      ],
      "metadata": {
        "id": "VbVaM3v7jJyy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = text.lower()\n",
        "print (text)\n",
        "\n",
        "print(text.upper())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NeRfA79hjJVO",
        "outputId": "5e0e4c93-a1bc-49c0-aaac-755c0d8a0dcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this command lists all files that git is currently tracking in your repository, including those in subdirectories. to check what files are in a git repository (including those on github, once cloned locally), you can use the following commands in your terminal or command prompt. note: to use these commands, you need to have git installed and be within the local clone of your github repository in your terminal.\n",
            "THIS COMMAND LISTS ALL FILES THAT GIT IS CURRENTLY TRACKING IN YOUR REPOSITORY, INCLUDING THOSE IN SUBDIRECTORIES. TO CHECK WHAT FILES ARE IN A GIT REPOSITORY (INCLUDING THOSE ON GITHUB, ONCE CLONED LOCALLY), YOU CAN USE THE FOLLOWING COMMANDS IN YOUR TERMINAL OR COMMAND PROMPT. NOTE: TO USE THESE COMMANDS, YOU NEED TO HAVE GIT INSTALLED AND BE WITHIN THE LOCAL CLONE OF YOUR GITHUB REPOSITORY IN YOUR TERMINAL.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Stemming**"
      ],
      "metadata": {
        "id": "XaKIlmNNjVec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "word_stemmer = PorterStemmer()\n",
        "\n",
        "# Tokenize the text into words\n",
        "words_for_stemming = word_tokenize(text)\n",
        "\n",
        "# Apply stemming\n",
        "stemmed_words = [word_stemmer.stem(word) for word in words_for_stemming]\n",
        "\n",
        "print(\"Original words for stemming:\", words_for_stemming)\n",
        "print(\"Stemmed words:\", stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HV0mEwFckdcl",
        "outputId": "e430649b-0a60-44d7-9f24-1a6726d4f73c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original words for stemming: ['this', 'command', 'lists', 'all', 'files', 'that', 'git', 'is', 'currently', 'tracking', 'in', 'your', 'repository', ',', 'including', 'those', 'in', 'subdirectories', '.', 'to', 'check', 'what', 'files', 'are', 'in', 'a', 'git', 'repository', '(', 'including', 'those', 'on', 'github', ',', 'once', 'cloned', 'locally', ')', ',', 'you', 'can', 'use', 'the', 'following', 'commands', 'in', 'your', 'terminal', 'or', 'command', 'prompt', '.', 'note', ':', 'to', 'use', 'these', 'commands', ',', 'you', 'need', 'to', 'have', 'git', 'installed', 'and', 'be', 'within', 'the', 'local', 'clone', 'of', 'your', 'github', 'repository', 'in', 'your', 'terminal', '.']\n",
            "Stemmed words: ['thi', 'command', 'list', 'all', 'file', 'that', 'git', 'is', 'current', 'track', 'in', 'your', 'repositori', ',', 'includ', 'those', 'in', 'subdirectori', '.', 'to', 'check', 'what', 'file', 'are', 'in', 'a', 'git', 'repositori', '(', 'includ', 'those', 'on', 'github', ',', 'onc', 'clone', 'local', ')', ',', 'you', 'can', 'use', 'the', 'follow', 'command', 'in', 'your', 'termin', 'or', 'command', 'prompt', '.', 'note', ':', 'to', 'use', 'these', 'command', ',', 'you', 'need', 'to', 'have', 'git', 'instal', 'and', 'be', 'within', 'the', 'local', 'clone', 'of', 'your', 'github', 'repositori', 'in', 'your', 'termin', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Lemmatization**"
      ],
      "metadata": {
        "id": "1WquAWbZk3mq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "word_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "word_lemmatizer.lemmatize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "lkx3TLpulAXJ",
        "outputId": "85ef811c-ae08-4eb9-eeff-eb0d4681f921"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'this command lists all files that git is currently tracking in your repository, including those in subdirectories. to check what files are in a git repository (including those on github, once cloned locally), you can use the following commands in your terminal or command prompt. note: to use these commands, you need to have git installed and be within the local clone of your github repository in your terminal.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. POS tagging**"
      ],
      "metadata": {
        "id": "WIdH0zkZlcKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "from nltk import word_tokenize, pos_tag\n",
        "print(pos_tag(word_tokenize(text)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0U-Sy7IlfHh",
        "outputId": "5e10f325-ad25-4516-a992-c21261d5ed1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('this', 'DT'), ('command', 'NN'), ('lists', 'VBZ'), ('all', 'DT'), ('files', 'NNS'), ('that', 'IN'), ('git', 'NN'), ('is', 'VBZ'), ('currently', 'RB'), ('tracking', 'VBG'), ('in', 'IN'), ('your', 'PRP$'), ('repository', 'NN'), (',', ','), ('including', 'VBG'), ('those', 'DT'), ('in', 'IN'), ('subdirectories', 'NNS'), ('.', '.'), ('to', 'TO'), ('check', 'VB'), ('what', 'WP'), ('files', 'NNS'), ('are', 'VBP'), ('in', 'IN'), ('a', 'DT'), ('git', 'JJ'), ('repository', 'NN'), ('(', '('), ('including', 'VBG'), ('those', 'DT'), ('on', 'IN'), ('github', 'NNS'), (',', ','), ('once', 'RB'), ('cloned', 'VBN'), ('locally', 'RB'), (')', ')'), (',', ','), ('you', 'PRP'), ('can', 'MD'), ('use', 'VB'), ('the', 'DT'), ('following', 'JJ'), ('commands', 'NNS'), ('in', 'IN'), ('your', 'PRP$'), ('terminal', 'NN'), ('or', 'CC'), ('command', 'NN'), ('prompt', 'NN'), ('.', '.'), ('note', 'NN'), (':', ':'), ('to', 'TO'), ('use', 'VB'), ('these', 'DT'), ('commands', 'NNS'), (',', ','), ('you', 'PRP'), ('need', 'VBP'), ('to', 'TO'), ('have', 'VB'), ('git', 'VBN'), ('installed', 'VBN'), ('and', 'CC'), ('be', 'VB'), ('within', 'IN'), ('the', 'DT'), ('local', 'JJ'), ('clone', 'NN'), ('of', 'IN'), ('your', 'PRP$'), ('github', 'JJ'), ('repository', 'NN'), ('in', 'IN'), ('your', 'PRP$'), ('terminal', 'NN'), ('.', '.')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TF-IDF example**"
      ],
      "metadata": {
        "id": "l5RROueSrw6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# list of text documents\n",
        "text = ['It is our first example in our course']\n",
        "# create the transform\n",
        "vectorizer = TfidfVectorizer()\n",
        "# tokenize and build vocab\n",
        "vectorizer.fit(text)\n",
        "# summarize\n",
        "print(vectorizer.vocabulary_)\n",
        "# encode document\n",
        "vector = vectorizer.transform(text)\n",
        "# summarize encoded vector\n",
        "print(vector.toarray())"
      ],
      "metadata": {
        "id": "EbUJuKXVruOt",
        "outputId": "a5cde215-1a16-4b1c-e388-886764aeb849",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'it': 5, 'is': 4, 'our': 6, 'first': 2, 'example': 1, 'in': 3, 'course': 0}\n",
            "[[0.31622777 0.31622777 0.31622777 0.31622777 0.31622777 0.31622777\n",
            "  0.63245553]]\n"
          ]
        }
      ]
    }
  ]
}