{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMtyu7PKDAC8WtT2oNCnbcr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/balkisoues/text_mining/blob/main/TM_cours.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Tokenization**\n",
        "\n"
      ],
      "metadata": {
        "id": "rNvm36mDh0gn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-D03aJAbxIC",
        "outputId": "5ff7c69e-5409-469a-d48f-3c39c3e7c37b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "text = 'This command lists all files that Git is currently tracking in your repository, including those in subdirectories. To check what files are in a Git repository (including those on GitHub, once cloned locally), you can use the following commands in your terminal or command prompt. Note: To use these commands, you need to have Git installed and be within the local clone of your GitHub repository in your terminal.'"
      ],
      "metadata": {
        "id": "XHrHbShNhD4a"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsgTmw85hXZW",
        "outputId": "f1aab8b0-255c-496e-9586-398630478196"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This command lists all files that Git is currently tracking in your repository, including those in subdirectories.',\n",
              " 'To check what files are in a Git repository (including those on GitHub, once cloned locally), you can use the following commands in your terminal or command prompt.',\n",
              " 'Note: To use these commands, you need to have Git installed and be within the local clone of your GitHub repository in your terminal.']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "text = \"Let us understand the difference between sentence & word tokenizer. It is going to be a simple example.\"\n",
        "\n",
        "sent_tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TY3IgI3jhghX",
        "outputId": "f65c0d89-e833-4679-efbd-30d4c0a279ef"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Let us understand the difference between sentence & word tokenizer.',\n",
              " 'It is going to be a simple example.']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "word_tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aju-ysuUh-OX",
        "outputId": "3c427d3f-c517-4cc2-d7cb-9810dab42bd9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This',\n",
              " 'command',\n",
              " 'lists',\n",
              " 'all',\n",
              " 'files',\n",
              " 'that',\n",
              " 'Git',\n",
              " 'is',\n",
              " 'currently',\n",
              " 'tracking',\n",
              " 'in',\n",
              " 'your',\n",
              " 'repository',\n",
              " ',',\n",
              " 'including',\n",
              " 'those',\n",
              " 'in',\n",
              " 'subdirectories',\n",
              " '.',\n",
              " 'To',\n",
              " 'check',\n",
              " 'what',\n",
              " 'files',\n",
              " 'are',\n",
              " 'in',\n",
              " 'a',\n",
              " 'Git',\n",
              " 'repository',\n",
              " '(',\n",
              " 'including',\n",
              " 'those',\n",
              " 'on',\n",
              " 'GitHub',\n",
              " ',',\n",
              " 'once',\n",
              " 'cloned',\n",
              " 'locally',\n",
              " ')',\n",
              " ',',\n",
              " 'you',\n",
              " 'can',\n",
              " 'use',\n",
              " 'the',\n",
              " 'following',\n",
              " 'commands',\n",
              " 'in',\n",
              " 'your',\n",
              " 'terminal',\n",
              " 'or',\n",
              " 'command',\n",
              " 'prompt',\n",
              " '.',\n",
              " 'Note',\n",
              " ':',\n",
              " 'To',\n",
              " 'use',\n",
              " 'these',\n",
              " 'commands',\n",
              " ',',\n",
              " 'you',\n",
              " 'need',\n",
              " 'to',\n",
              " 'have',\n",
              " 'Git',\n",
              " 'installed',\n",
              " 'and',\n",
              " 'be',\n",
              " 'within',\n",
              " 'the',\n",
              " 'local',\n",
              " 'clone',\n",
              " 'of',\n",
              " 'your',\n",
              " 'GitHub',\n",
              " 'repository',\n",
              " 'in',\n",
              " 'your',\n",
              " 'terminal',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Filter stop words**"
      ],
      "metadata": {
        "id": "VpnRlXe6iTP5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "print (\" english stop words \" , stopwords.words('english') )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2Kf4uXjiCA8",
        "outputId": "91cc51b3-4067-4e5a-9df8-c13762ec4a98"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " english stop words  ['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Applying stop words removal to text**"
      ],
      "metadata": {
        "id": "bc6igZOqi_Lk"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbb18de7",
        "outputId": "eb2b9173-6e6f-42d9-c559-d6075638223e"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Tokenize the text into words\n",
        "words = word_tokenize(text)\n",
        "\n",
        "# Get English stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Filter out stop words\n",
        "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "print(\"Original words:\", words)\n",
        "print(\"Filtered words (after stop word removal):\")\n",
        "print(filtered_words)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original words: ['This', 'command', 'lists', 'all', 'files', 'that', 'Git', 'is', 'currently', 'tracking', 'in', 'your', 'repository', ',', 'including', 'those', 'in', 'subdirectories', '.', 'To', 'check', 'what', 'files', 'are', 'in', 'a', 'Git', 'repository', '(', 'including', 'those', 'on', 'GitHub', ',', 'once', 'cloned', 'locally', ')', ',', 'you', 'can', 'use', 'the', 'following', 'commands', 'in', 'your', 'terminal', 'or', 'command', 'prompt', '.', 'Note', ':', 'To', 'use', 'these', 'commands', ',', 'you', 'need', 'to', 'have', 'Git', 'installed', 'and', 'be', 'within', 'the', 'local', 'clone', 'of', 'your', 'GitHub', 'repository', 'in', 'your', 'terminal', '.']\n",
            "Filtered words (after stop word removal):\n",
            "['command', 'lists', 'files', 'Git', 'currently', 'tracking', 'repository', ',', 'including', 'subdirectories', '.', 'check', 'files', 'Git', 'repository', '(', 'including', 'GitHub', ',', 'cloned', 'locally', ')', ',', 'use', 'following', 'commands', 'terminal', 'command', 'prompt', '.', 'Note', ':', 'use', 'commands', ',', 'need', 'Git', 'installed', 'within', 'local', 'clone', 'GitHub', 'repository', 'terminal', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Capitalization**"
      ],
      "metadata": {
        "id": "VbVaM3v7jJyy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = text.lower()\n",
        "print (text)\n",
        "\n",
        "print(text.upper())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NeRfA79hjJVO",
        "outputId": "5e0e4c93-a1bc-49c0-aaac-755c0d8a0dcb"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this command lists all files that git is currently tracking in your repository, including those in subdirectories. to check what files are in a git repository (including those on github, once cloned locally), you can use the following commands in your terminal or command prompt. note: to use these commands, you need to have git installed and be within the local clone of your github repository in your terminal.\n",
            "THIS COMMAND LISTS ALL FILES THAT GIT IS CURRENTLY TRACKING IN YOUR REPOSITORY, INCLUDING THOSE IN SUBDIRECTORIES. TO CHECK WHAT FILES ARE IN A GIT REPOSITORY (INCLUDING THOSE ON GITHUB, ONCE CLONED LOCALLY), YOU CAN USE THE FOLLOWING COMMANDS IN YOUR TERMINAL OR COMMAND PROMPT. NOTE: TO USE THESE COMMANDS, YOU NEED TO HAVE GIT INSTALLED AND BE WITHIN THE LOCAL CLONE OF YOUR GITHUB REPOSITORY IN YOUR TERMINAL.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Stemming**"
      ],
      "metadata": {
        "id": "XaKIlmNNjVec"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "snNP8m1FjVKw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}