{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMVhub4mtTFLeslIif9Qt2R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rouakhadhraoui/Text-Mining-Labs-/blob/main/Text_summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Objectif du Lab :\n",
        "Comparer trois algorithmes de résumé extractif :\n",
        "\n",
        "TF-IDF\n",
        "TextRank\n",
        "LSA\n",
        "\n",
        "→ Pour chacun, générer un résumé à partir du texte fourni,\n",
        "→ Puis évaluer sa qualité avec le score BLEU, en le comparant à un résumé de référence humain :\n"
      ],
      "metadata": {
        "id": "YxV2DugFFuwv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Étape 0 : Préparer l’environnement"
      ],
      "metadata": {
        "id": "B21TWDf_Gkg-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lq8w9a_uDcgq",
        "outputId": "2e7c207e-ba0a-474b-96a1-d45d014238b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: sumy in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
            "Requirement already satisfied: yake in /usr/local/lib/python3.12/dist-packages (0.6.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: docopt<0.7,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from sumy) (0.6.2)\n",
            "Requirement already satisfied: breadability>=0.1.20 in /usr/local/lib/python3.12/dist-packages (from sumy) (0.1.20)\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.12/dist-packages (from sumy) (2.32.4)\n",
            "Requirement already satisfied: pycountry>=18.2.23 in /usr/local/lib/python3.12/dist-packages (from sumy) (24.6.1)\n",
            "Requirement already satisfied: jellyfish in /usr/local/lib/python3.12/dist-packages (from yake) (1.2.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from yake) (3.5)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.12/dist-packages (from yake) (2.0.2)\n",
            "Requirement already satisfied: segtok in /usr/local/lib/python3.12/dist-packages (from yake) (1.5.11)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from yake) (0.9.0)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.12/dist-packages (from breadability>=0.1.20->sumy) (5.2.0)\n",
            "Requirement already satisfied: lxml>=2.0 in /usr/local/lib/python3.12/dist-packages (from breadability>=0.1.20->sumy) (5.4.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.7.0->sumy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.7.0->sumy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.7.0->sumy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.7.0->sumy) (2025.10.5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# Installer les bibliothèques nécessaires\n",
        "!pip install nltk sumy yake\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')  # nouvelle ressource depuis NLTK v3.9+\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Étape 1 : installer dépendances + imports + texte / référence"
      ],
      "metadata": {
        "id": "qDnqqat3GgCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Install libs (Colab)\n",
        "!pip install -q nltk scikit-learn networkx numpy gensim sumy\n",
        "\n",
        "# 2) Imports\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.lsa import LsaSummarizer\n",
        "\n",
        "# Ensure punkt tokenizer\n",
        "nltk.download('punkt')\n",
        "\n",
        "# 3) Input text and reference (given in the lab)\n",
        "text = (\"You can learn a lot about yourself through travelling. You can observe how you feel being far from your \"\n",
        "\"country. You will find out how you feel about your homeland. You will realize how you really feel about \"\n",
        "\"foreign people. You will find out how much you know/do not know about the world. You will be able to \"\n",
        "\"observe how you react in completely new situations. You will test your language, orientational and \"\n",
        "\"social skills. You will not be the same person after returning home. During travelling you will meet \"\n",
        "\"people that are very different from you. If you travel enough, you will learn to accept and appreciate \"\n",
        "\"these differences. Traveling makes you more open and accepting.\")\n",
        "\n",
        "reference = \"Travelling teaches you about yourself. It makes you more tolerant.\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHxOjvzDG4tu",
        "outputId": "9639081d-2b8b-456a-87f4-f6bede881df7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le texte source est un paragraphe sur les bienfaits personnels du voyage.\n",
        "Le résumé de référence, court et sémantiquement riche, servira de vérité terrain pour évaluer nos algorithmes."
      ],
      "metadata": {
        "id": "xc4kC6trG_1u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Étape 2 : tokenisation des phrases (prétraitement)"
      ],
      "metadata": {
        "id": "zThdIccWHMJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = sent_tokenize(text)\n",
        "print(\"Number of sentences:\", len(sentences))\n",
        "for i,s in enumerate(sentences,1):\n",
        "    print(f\"{i}. {s}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoHY_K_lHOyX",
        "outputId": "160d87cf-f537-4e1b-cef9-105efc9ffde0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sentences: 11\n",
            "1. You can learn a lot about yourself through travelling.\n",
            "2. You can observe how you feel being far from your country.\n",
            "3. You will find out how you feel about your homeland.\n",
            "4. You will realize how you really feel about foreign people.\n",
            "5. You will find out how much you know/do not know about the world.\n",
            "6. You will be able to observe how you react in completely new situations.\n",
            "7. You will test your language, orientational and social skills.\n",
            "8. You will not be the same person after returning home.\n",
            "9. During travelling you will meet people that are very different from you.\n",
            "10. If you travel enough, you will learn to accept and appreciate these differences.\n",
            "11. Traveling makes you more open and accepting.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "c’est l’unité sur laquelle on fait de l’extractive summarization."
      ],
      "metadata": {
        "id": "9je8Sy4LHWQh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Étape 3 : Résumé avec TF-IDF"
      ],
      "metadata": {
        "id": "GvbKG131HaXl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tfidf_summarize(sentences, top_k=2):\n",
        "    # Build TF-IDF over sentences (each sentence = document)\n",
        "    vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1,2))\n",
        "    tfidf = vectorizer.fit_transform(sentences)\n",
        "    # Score sentence by sum of its TF-IDF weights\n",
        "    scores = tfidf.sum(axis=1).A1\n",
        "    top_idx = np.argsort(scores)[-top_k:]  # indices of top scoring sentences (unsorted)\n",
        "    # Preserve original order of sentences in the final summary\n",
        "    summary = ' '.join([sentences[i] for i in sorted(top_idx)])\n",
        "    return summary, scores, sorted(top_idx)\n",
        "\n",
        "tfidf_sum, tfidf_scores, tfidf_idx = tfidf_summarize(sentences, top_k=2)\n",
        "print(\"TF-IDF Summary:\\n\", tfidf_sum)\n",
        "print(\"Selected sentence indices:\", tfidf_idx)\n",
        "print(\"Sentence scores:\", np.round(tfidf_scores,3))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VScl8y_vHf5V",
        "outputId": "721db28b-adc6-48ec-c357-3fdb30bb01de"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Summary:\n",
            " You will be able to observe how you react in completely new situations. You will test your language, orientational and social skills.\n",
            "Selected sentence indices: [np.int64(5), np.int64(6)]\n",
            "Sentence scores: [2.23  2.633 1.718 2.988 1.89  3.314 3.    2.236 2.64  2.997 2.646]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF attribue un poids aux mots selon leur fréquence dans la phrase et leur rareté globale.\n",
        "Les phrases avec les mots les plus \"importants\" sont sélectionnées.\n",
        "→ Avantage : simple, rapide.\n",
        "→ Limite : ne comprend pas le sens, sensible au bruit."
      ],
      "metadata": {
        "id": "b6WyJHNNHwDD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Étape 4 : Résumé avec TextRank"
      ],
      "metadata": {
        "id": "gEqx2ocfII9R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def textrank_summarize(sentences, top_k=2):\n",
        "    # Vectorize sentences (TF-IDF)\n",
        "    vectorizer = TfidfVectorizer(stop_words='english')\n",
        "    tfidf = vectorizer.fit_transform(sentences)\n",
        "    # Cosine similarity matrix\n",
        "    sim_mat = cosine_similarity(tfidf)\n",
        "    # Build graph and rank\n",
        "    nx_graph = nx.from_numpy_array(sim_mat)\n",
        "    try:\n",
        "        scores = nx.pagerank_numpy(nx_graph, weight='weight')\n",
        "    except:\n",
        "        scores = nx.pagerank(nx_graph, weight='weight')\n",
        "    # top-k sentences by score\n",
        "    ranked = sorted(((scores[i], i) for i in range(len(sentences))), reverse=True)\n",
        "    top_idx = [idx for (_, idx) in ranked[:top_k]]\n",
        "    summary = ' '.join([sentences[i] for i in sorted(top_idx)])\n",
        "    return summary, scores, sorted(top_idx)\n",
        "\n",
        "tr_sum, tr_scores, tr_idx = textrank_summarize(sentences, top_k=2)\n",
        "print(\"TextRank Summary:\\n\", tr_sum)\n",
        "print(\"Selected sentence indices:\", tr_idx)\n",
        "print(\"PageRank scores:\", {i:round(tr_scores[i],3) for i in tr_scores})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJSOgMIQIMww",
        "outputId": "33b93442-4611-478b-c939-7c18ec6d511c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TextRank Summary:\n",
            " You can observe how you feel being far from your country. You will realize how you really feel about foreign people.\n",
            "Selected sentence indices: [1, 3]\n",
            "PageRank scores: {0: 0.095, 1: 0.097, 2: 0.091, 3: 0.095, 4: 0.091, 5: 0.083, 6: 0.091, 7: 0.091, 8: 0.091, 9: 0.085, 10: 0.091}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TextRank parvient à bien condenser le texte tout en conservant ses idées principales grâce à sa vision \"relationnelle\" du document. Il est particulièrement efficace pour des textes informatifs ou descriptifs où la redondance est forte."
      ],
      "metadata": {
        "id": "-27rrodsNNpX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Étape 5 : Résumé avec LSA"
      ],
      "metadata": {
        "id": "TX1PeT6YIjNB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.lsa import LsaSummarizer\n",
        "\n",
        "def lsa_summarize(text, top_k=2):\n",
        "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
        "    summarizer = LsaSummarizer()\n",
        "    sents = summarizer(parser.document, top_k)\n",
        "    summary = ' '.join(str(s) for s in sents)\n",
        "    return summary, list(sents)\n",
        "\n",
        "lsa_sum, lsa_sents = lsa_summarize(text, top_k=2)\n",
        "print(\"LSA Summary:\\n\", lsa_sum)\n",
        "print(\"LSA selected sentences:\")\n",
        "for s in lsa_sents:\n",
        "    print(\"-\", s)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_nRH5oZIoYl",
        "outputId": "095f0901-7c01-469f-f9c4-3a05a856058f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSA Summary:\n",
            " You can learn a lot about yourself through travelling. You will not be the same person after returning home.\n",
            "LSA selected sentences:\n",
            "- You can learn a lot about yourself through travelling.\n",
            "- You will not be the same person after returning home.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSA utilise la décomposition SVD pour extraire des concepts latents, ce qui permet de gérer la synonymie et le bruit.\n",
        "Il identifie les phrases qui couvrent le mieux les concepts principaux du texte.\n",
        "→ Avantage : plus sémantique, robuste aux variations lexicales."
      ],
      "metadata": {
        "id": "E3tGtjN-IuVu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Étape 6 : calculer BLEU entre chaque résumé et la référence"
      ],
      "metadata": {
        "id": "DyISi8exIwor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "smooth = SmoothingFunction().method1\n",
        "\n",
        "def bleu_score(candidate, reference):\n",
        "    cand_toks = [w.lower() for w in word_tokenize(candidate)]\n",
        "    ref_toks = [w.lower() for w in word_tokenize(reference)]\n",
        "    return sentence_bleu([ref_toks], cand_toks, smoothing_function=smooth)\n",
        "\n",
        "print(\"Reference:\", reference, \"\\n\")\n",
        "print(\"TF-IDF BLEU:\", bleu_score(tfidf_sum, reference))\n",
        "print(\"TextRank BLEU:\", bleu_score(tr_sum, reference))\n",
        "print(\"LSA BLEU:\", bleu_score(lsa_sum, reference))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqcLgPhqI1EA",
        "outputId": "00237278-f870-4fce-bdc0-a95a10d64150"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reference: Travelling teaches you about yourself. It makes you more tolerant. \n",
            "\n",
            "TF-IDF BLEU: 0.010713701843513142\n",
            "TextRank BLEU: 0.012384901282810543\n",
            "LSA BLEU: 0.02642138995497447\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le score BLEU le plus élevé indique le résumé le plus proche (lexicalement) de la référence humaine.\n",
        "Cependant, un faible score BLEU ne signifie pas nécessairement un mauvais résumé : BLEU ne mesure pas la cohérence sémantique, seulement le recouvrement de mots.\n",
        "Par exemple, un résumé disant \"Traveling helps you discover yourself and become more open-minded\" est excellent, mais aura un BLEU faible car il n’utilise pas \"tolerant\"."
      ],
      "metadata": {
        "id": "VHh1XBhXI9JU"
      }
    }
  ]
}